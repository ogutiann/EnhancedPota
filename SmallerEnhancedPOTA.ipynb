{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyObdqJG+jHgPfLSrTAlwt95",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ogutiann/EnhancedPota/blob/main/SmallerEnhancedPOTA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuplOsfWF9HB",
        "outputId": "1110a6ee-8cc4-40a7-9403-81ca381cb3a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch\n",
            "  Using cached torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting filelock (from torch)\n",
            "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting typing-extensions>=4.10.0 (from torch)\n",
            "  Using cached typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch)\n",
            "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch)\n",
            "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Using cached fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
            "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
            "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.4.0 (from torch)\n",
            "  Using cached triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting setuptools>=40.8.0 (from triton==3.4.0->torch)\n",
            "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
            "  Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Using cached torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n",
            "Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "Using cached triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n",
            "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
            "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Using cached fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
            "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
            "Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "Installing collected packages: nvidia-cusparselt-cu12, mpmath, typing-extensions, sympy, setuptools, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.7.1\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.7.1:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.7.1\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.14.1\n",
            "    Uninstalling typing_extensions-4.14.1:\n",
            "      Successfully uninstalled typing_extensions-4.14.1\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.14.0\n",
            "    Uninstalling sympy-1.14.0:\n",
            "      Successfully uninstalled sympy-1.14.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 80.9.0\n",
            "    Uninstalling setuptools-80.9.0:\n",
            "      Successfully uninstalled setuptools-80.9.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.8.90\n",
            "    Uninstalling nvidia-nvtx-cu12-12.8.90:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.8.90\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
            "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
            "  Attempting uninstall: nvidia-cufile-cu12\n",
            "    Found existing installation: nvidia-cufile-cu12 1.13.1.3\n",
            "    Uninstalling nvidia-cufile-cu12-1.13.1.3:\n",
            "      Successfully uninstalled nvidia-cufile-cu12-1.13.1.3\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.8.90\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.8.90:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.90\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.93\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.8.93:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.93\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.8.90\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.8.90:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.90\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.5\n",
            "    Uninstalling networkx-3.5:\n",
            "      Successfully uninstalled networkx-3.5\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.7.0\n",
            "    Uninstalling fsspec-2025.7.0:\n",
            "      Successfully uninstalled fsspec-2025.7.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.18.0\n",
            "    Uninstalling filelock-3.18.0:\n",
            "      Successfully uninstalled filelock-3.18.0\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.4.0\n",
            "    Uninstalling triton-3.4.0:\n",
            "      Successfully uninstalled triton-3.4.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.8.0\n",
            "    Uninstalling torch-2.8.0:\n",
            "      Successfully uninstalled torch-2.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch --upgrade --force-reinstall\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pickle\n",
        "import math\n",
        "\n",
        "# GPU diagnostics\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
        "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Configuration\n",
        "DEMO_MODE = True\n",
        "SAVE_DATA = True\n",
        "if DEMO_MODE:\n",
        "    N_WORKERS = 500\n",
        "    N_TASKS = 1000\n",
        "    RL_EPOCHS = 5\n",
        "    VAE_EPOCHS = 10\n",
        "else:\n",
        "    N_WORKERS = 5000\n",
        "    N_TASKS = 10000\n",
        "    RL_EPOCHS = 20\n",
        "    VAE_EPOCHS = 50\n",
        "\n",
        "CAPACITY = 3\n",
        "GPU_ACCELERATED = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if GPU_ACCELERATED else \"cpu\")\n",
        "\n",
        "print(f\"⚙️ Configuration: {N_WORKERS} workers, {N_TASKS} tasks\")\n",
        "print(f\"🚀 Using {'GPU: ' + torch.cuda.get_device_name(0) if GPU_ACCELERATED else 'CPU'}\")\n",
        "\n",
        "# Set random seeds\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Load and preprocess air quality dataset\n",
        "def load_air_quality_data():\n",
        "    air_quality = None\n",
        "    try:\n",
        "        # Using the provided sample structure\n",
        "        air_quality = pd.DataFrame({\n",
        "            'geo_entity_name': ['Flushing and Whitestone (CD7)', 'Upper West Side (CD7)', 'Rockaway and Broad Channel (CD14)'],\n",
        "            'data_valuemessage': [23.97, 27.42, 12.55]\n",
        "        })\n",
        "\n",
        "        # For full dataset, you would use:\n",
        "        # air_quality = pd.read_csv('air_quality.csv')\n",
        "        # air_quality = air_quality[['Geo Place Name', 'Data Value']]\n",
        "        # air_quality.columns = ['geo_entity_name', 'data_valuemessage']\n",
        "\n",
        "        print(\"✅ Loaded air quality data\")\n",
        "        print(f\"Found {len(air_quality)} unique geographic entities\")\n",
        "        return air_quality\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Air quality data loading failed: {e}\")\n",
        "        return None\n",
        "\n",
        "AIR_QUALITY_DF = load_air_quality_data()\n",
        "\n",
        "# Data persistence\n",
        "def save_synthetic_data(workers, tasks, filename=\"synthetic_data.pkl\"):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump({'workers': workers, 'tasks': tasks}, f)\n",
        "\n",
        "def load_synthetic_data(filename=\"synthetic_data.pkl\"):\n",
        "    try:\n",
        "        with open(filename, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "            return data['workers'], data['tasks']\n",
        "    except FileNotFoundError:\n",
        "        return None\n",
        "\n",
        "######################\n",
        "### 1. System Model ##\n",
        "######################\n",
        "class Worker:\n",
        "    def __init__(self, id, real_world=False):\n",
        "        self.id = id\n",
        "        self.real_world = real_world\n",
        "\n",
        "        if real_world and AIR_QUALITY_DF is not None:\n",
        "            self.location = self.generate_nyc_location()\n",
        "            self.skill = self.generate_nyc_skills()\n",
        "            self.speed = self.generate_nyc_speed()\n",
        "        else:\n",
        "            self.location = np.array([np.random.uniform(0,100), np.random.uniform(0,100)])\n",
        "            self.skill = np.array([np.random.uniform(2,5), np.random.uniform(2,5)])\n",
        "            self.speed = np.random.uniform(0.5, 2)\n",
        "\n",
        "        self.weights = np.random.dirichlet(np.ones(3))\n",
        "        self.reputation = np.random.uniform(0.7, 0.95)\n",
        "        self.utility_history = []\n",
        "        self.assigned_tasks = []\n",
        "\n",
        "    def generate_nyc_location(self):\n",
        "        borough_probs = [0.3, 0.3, 0.2, 0.15, 0.05]\n",
        "        borough = np.random.choice(5, p=borough_probs)\n",
        "        centroids = {\n",
        "            0: [40.7831, -73.9712],\n",
        "            1: [40.6782, -73.9442],\n",
        "            2: [40.7282, -73.7949],\n",
        "            3: [40.8448, -73.8648],\n",
        "            4: [40.5795, -74.1502]\n",
        "        }\n",
        "        lat = centroids[borough][0] + np.random.uniform(-0.05, 0.05)\n",
        "        lon = centroids[borough][1] + np.random.uniform(-0.05, 0.05)\n",
        "        return np.array([lat, lon])\n",
        "\n",
        "    def generate_nyc_skills(self):\n",
        "        if self.location[0] > 40.75:\n",
        "            return np.array([np.random.uniform(4, 5), np.random.uniform(4, 5)])\n",
        "        elif self.location[0] > 40.65:\n",
        "            return np.array([np.random.uniform(3.5, 4.5), np.random.uniform(3.5, 4.5)])\n",
        "        else:\n",
        "            return np.array([np.random.uniform(3, 4), np.random.uniform(3, 4)])\n",
        "\n",
        "    def generate_nyc_speed(self):\n",
        "        if self.location[0] > 40.75:\n",
        "            return np.random.uniform(0.5, 1.2)\n",
        "        elif self.location[0] > 40.65:\n",
        "            return np.random.uniform(1.0, 1.8)\n",
        "        else:\n",
        "            return np.random.uniform(1.5, 2.0)\n",
        "\n",
        "    def travel_time(self, task_location):\n",
        "        # Haversine distance calculation\n",
        "        lat1, lon1 = np.radians(self.location)\n",
        "        lat2, lon2 = np.radians(task_location)\n",
        "\n",
        "        dlat = lat2 - lat1\n",
        "        dlon = lon2 - lon1\n",
        "\n",
        "        a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
        "        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
        "        distance = 6371 * c  # Earth radius in km\n",
        "\n",
        "        return max(0.1, distance / self.speed)\n",
        "\n",
        "class Task:\n",
        "    def __init__(self, id, real_world=False):\n",
        "        self.id = id\n",
        "        self.real_world = real_world\n",
        "\n",
        "        if real_world and AIR_QUALITY_DF is not None:\n",
        "            self.location, self.reward, self.difficulty, self.required_skill = self.generate_air_quality_task()\n",
        "        else:\n",
        "            self.location = np.array([np.random.uniform(0,100), np.random.uniform(0,100)])\n",
        "            self.reward = np.random.uniform(5,20)\n",
        "            self.difficulty = np.random.uniform(1,10)\n",
        "            self.required_skill = np.array([np.random.uniform(1,4), np.random.uniform(1,4)])\n",
        "\n",
        "        self.deadline = np.random.randint(10,60)\n",
        "\n",
        "    def generate_air_quality_task(self):\n",
        "        \"\"\"Generate task based on NYC air quality data\"\"\"\n",
        "        borough_centroids = {\n",
        "            \"Manhattan\": [40.7831, -73.9712],\n",
        "            \"Brooklyn\": [40.6782, -73.9442],\n",
        "            \"Queens\": [40.7282, -73.7949],\n",
        "            \"Bronx\": [40.8448, -73.8648],\n",
        "            \"Staten Island\": [40.5795, -74.1502]\n",
        "        }\n",
        "\n",
        "        # Select random geographic entity\n",
        "        geo_entity = np.random.choice(AIR_QUALITY_DF['geo_entity_name'])\n",
        "        b_data = AIR_QUALITY_DF[AIR_QUALITY_DF['geo_entity_name'] == geo_entity].iloc[0]\n",
        "        air_quality_value = b_data['data_valuemessage']\n",
        "\n",
        "        # Normalize air quality values to [1,10] range\n",
        "        aq_min, aq_max = AIR_QUALITY_DF['data_valuemessage'].min(), AIR_QUALITY_DF['data_valuemessage'].max()\n",
        "        normalized_aq = (air_quality_value - aq_min) / (aq_max - aq_min) * 9 + 1\n",
        "        difficulty = np.clip(normalized_aq, 1, 10)\n",
        "\n",
        "        # Map to nearest borough\n",
        "        min_dist = float('inf')\n",
        "        closest_borough = \"Manhattan\"\n",
        "        for borough, centroid in borough_centroids.items():\n",
        "            dist = np.linalg.norm(np.array(centroid) - np.array([40.7128, -74.0060]))\n",
        "            if dist < min_dist:\n",
        "                min_dist = dist\n",
        "                closest_borough = borough\n",
        "\n",
        "        location = np.array(borough_centroids[closest_borough])\n",
        "\n",
        "        # Cap required skills to worker capabilities\n",
        "        required_skill = np.array([3.0 + difficulty/3, 3.5 + difficulty/3])\n",
        "        required_skill = np.clip(required_skill, 0, 5)  # Cap at max worker skill\n",
        "\n",
        "        reward = 10 + difficulty * 2\n",
        "        return location, reward, difficulty, required_skill\n",
        "\n",
        "################################\n",
        "### 2. Deep-Quality Reputation ##\n",
        "################################\n",
        "class QualityVAE(nn.Module):\n",
        "    def __init__(self, input_dim=3, latent_dim=8):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 32), nn.ReLU(),\n",
        "            nn.Linear(32, 16), nn.ReLU()\n",
        "        )\n",
        "        self.mu = nn.Linear(16, latent_dim)\n",
        "        self.logvar = nn.Linear(16, latent_dim)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 16), nn.ReLU(),\n",
        "            nn.Linear(16, 32), nn.ReLU(),\n",
        "            nn.Linear(32, input_dim), nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = self.mu(h), self.logvar(h)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decoder(z), mu, logvar\n",
        "\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + 0.5 * KLD\n",
        "\n",
        "################################################\n",
        "### 3. Federated RL for Preference Optimization #\n",
        "################################################\n",
        "class LSTMPolicy(nn.Module):\n",
        "    def __init__(self, input_size=5, hidden_size=32, output_size=3):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.linear(x[:, -1, :])\n",
        "        return torch.softmax(x, dim=-1)\n",
        "\n",
        "def federated_average(models):\n",
        "    global_dict = {}\n",
        "    for key in models[0].state_dict().keys():\n",
        "        global_dict[key] = torch.stack([m.state_dict()[key] for m in models]).mean(0)\n",
        "    return global_dict\n",
        "\n",
        "def train_worker_rl(worker, tasks, epochs=RL_EPOCHS, batch_size=64, lr=0.005):\n",
        "    model = LSTMPolicy().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    n_samples = 1000 if not DEMO_MODE else 500\n",
        "    states = torch.randn(n_samples, 1, 5, device=device)\n",
        "    actions = torch.randint(0, 3, (n_samples,), device=device)\n",
        "    rewards = torch.rand(n_samples, device=device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        perm = torch.randperm(n_samples)\n",
        "        for i in range(0, n_samples, batch_size):\n",
        "            idx = perm[i:i+batch_size]\n",
        "            batch_s = states[idx]\n",
        "            batch_a = actions[idx]\n",
        "            batch_r = rewards[idx]\n",
        "\n",
        "            action_probs = model(batch_s)\n",
        "            selected_probs = action_probs[torch.arange(len(batch_a)), batch_a]\n",
        "            loss = -torch.log(selected_probs) * batch_r\n",
        "            loss = loss.mean()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "#############################################\n",
        "### 4. Vectorized Utility Calculations (Fixed) ##\n",
        "#############################################\n",
        "def vectorized_utilities(workers, tasks, beta=0.3, zeta=0.3, kappa=0.7):\n",
        "    worker_locs = np.array([w.location for w in workers])\n",
        "    task_locs = np.array([t.location for t in tasks])\n",
        "    n_workers = len(workers)\n",
        "    n_tasks = len(tasks)\n",
        "\n",
        "    # Initialize distance matrix\n",
        "    distances = np.zeros((n_workers, n_tasks))\n",
        "\n",
        "    # Calculate distances\n",
        "    for i, worker in enumerate(workers):\n",
        "        for j, task in enumerate(tasks):\n",
        "            if worker.real_world:\n",
        "                # Haversine distance for real-world locations\n",
        "                lat1, lon1 = np.radians(worker.location)\n",
        "                lat2, lon2 = np.radians(task.location)\n",
        "                dlat = lat2 - lat1\n",
        "                dlon = lon2 - lon1\n",
        "                a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
        "                c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
        "                distances[i, j] = 6371 * c\n",
        "            else:\n",
        "                # Euclidean distance for synthetic data\n",
        "                distances[i, j] = np.linalg.norm(worker.location - task.location)\n",
        "\n",
        "    # Vectorized calculations\n",
        "    speeds = np.array([w.speed for w in workers])\n",
        "    tau = distances / speeds[:, None]  # Shape: (n_workers, n_tasks)\n",
        "\n",
        "    worker_skills = np.array([w.skill for w in workers])\n",
        "    task_reqs = np.array([t.required_skill for t in tasks])\n",
        "\n",
        "    # Skill gap calculation\n",
        "    skill_gap = np.zeros((n_workers, n_tasks))\n",
        "    for i in range(n_workers):\n",
        "        for j in range(n_tasks):\n",
        "            skill_gap[i, j] = np.mean(worker_skills[i] - task_reqs[j])\n",
        "\n",
        "    worker_weights = np.array([w.weights for w in workers])\n",
        "    task_rewards = np.array([t.reward for t in tasks])\n",
        "    reputations = np.array([w.reputation for w in workers])\n",
        "\n",
        "    # Worker utilities\n",
        "    w_utils = (\n",
        "        worker_weights[:, 0][:, None] * (1 / (tau + 1e-8)) +\n",
        "        worker_weights[:, 1][:, None] * task_rewards[None, :] +\n",
        "        worker_weights[:, 2][:, None] * skill_gap +\n",
        "        beta * reputations[:, None]\n",
        "    )\n",
        "\n",
        "    # Task utilities\n",
        "    skill_mask = np.zeros((n_workers, n_tasks), dtype=bool)\n",
        "    for i in range(n_workers):\n",
        "        for j in range(n_tasks):\n",
        "            skill_mask[i, j] = np.all(worker_skills[i] >= task_reqs[j])\n",
        "\n",
        "    t_utils = kappa * (1 / (tau + 1e-8)) + zeta * reputations[:, None]\n",
        "    t_utils = np.where(skill_mask, t_utils, -np.inf)\n",
        "\n",
        "    return w_utils, t_utils\n",
        "\n",
        "#############################################\n",
        "### 5. Matching Algorithms Implementations ##\n",
        "#############################################\n",
        "def rags_matching(workers, tasks, capacity=CAPACITY, beta=0.3, zeta=0.3, kappa=0.7):\n",
        "    w_utils, t_utils = vectorized_utilities(workers, tasks, beta, zeta, kappa)\n",
        "    n_workers = len(workers)\n",
        "    n_tasks = len(tasks)\n",
        "\n",
        "    # Initialize data structures\n",
        "    proposals = np.zeros(n_workers, dtype=int)\n",
        "    worker_assignments = [[] for _ in range(n_workers)]\n",
        "    task_assignments = [[] for _ in range(n_tasks)]\n",
        "    unmatched = list(range(n_workers))\n",
        "\n",
        "    # Precompute preferences\n",
        "    w_prefs = np.argsort(-w_utils, axis=1)  # Worker preferences: tasks sorted by utility\n",
        "\n",
        "    # Matching loop\n",
        "    while unmatched:\n",
        "        i = unmatched.pop(0)\n",
        "        if proposals[i] >= n_tasks:\n",
        "            continue\n",
        "\n",
        "        j = w_prefs[i, proposals[i]]\n",
        "        proposals[i] += 1\n",
        "\n",
        "        if len(task_assignments[j]) < capacity:\n",
        "            worker_assignments[i].append(j)\n",
        "            task_assignments[j].append(i)\n",
        "        else:\n",
        "            current_workers = task_assignments[j]\n",
        "            current_utils = [t_utils[k, j] for k in current_workers]\n",
        "            min_util = min(current_utils)\n",
        "            worst_idx = current_workers[np.argmin(current_utils)]\n",
        "\n",
        "            if t_utils[i, j] > min_util:\n",
        "                # Replace worker\n",
        "                worker_assignments[worst_idx].remove(j)\n",
        "                task_assignments[j].remove(worst_idx)\n",
        "                unmatched.append(worst_idx)\n",
        "\n",
        "                worker_assignments[i].append(j)\n",
        "                task_assignments[j].append(i)\n",
        "            else:\n",
        "                unmatched.append(i)\n",
        "\n",
        "    return worker_assignments, task_assignments\n",
        "\n",
        "def classic_gs(workers, tasks, capacity=CAPACITY):\n",
        "    \"\"\"True Gale-Shapley implementation without reputation\"\"\"\n",
        "    n_workers = len(workers)\n",
        "    n_tasks = len(tasks)\n",
        "\n",
        "    # Precompute utilities\n",
        "    w_utils = np.zeros((n_workers, n_tasks))\n",
        "    t_utils = np.zeros((n_tasks, n_workers))\n",
        "\n",
        "    for i, worker in enumerate(workers):\n",
        "        for j, task in enumerate(tasks):\n",
        "            # Worker utility without reputation\n",
        "            tau = worker.travel_time(task.location)\n",
        "            skill_gap = np.mean(worker.skill - task.required_skill)\n",
        "            w_utils[i, j] = (worker.weights[0] * (1/tau) +\n",
        "                             worker.weights[1] * task.reward +\n",
        "                             worker.weights[2] * skill_gap)\n",
        "\n",
        "            # Task utility without reputation\n",
        "            if np.all(worker.skill >= task.required_skill):\n",
        "                t_utils[j, i] = 1/tau\n",
        "            else:\n",
        "                t_utils[j, i] = -np.inf\n",
        "\n",
        "    # Preference lists\n",
        "    w_prefs = [np.argsort(-w_utils[i]) for i in range(n_workers)]\n",
        "    t_prefs = [np.argsort(-t_utils[j]) for j in range(n_tasks)]\n",
        "\n",
        "    # Initialize\n",
        "    proposals = np.zeros(n_workers, dtype=int)\n",
        "    worker_assignments = [[] for _ in range(n_workers)]\n",
        "    task_assignments = [[] for _ in range(n_tasks)]\n",
        "    unmatched = list(range(n_workers))\n",
        "\n",
        "    # Matching loop\n",
        "    while unmatched:\n",
        "        i = unmatched.pop(0)\n",
        "        if proposals[i] >= n_tasks:\n",
        "            continue\n",
        "\n",
        "        j = w_prefs[i][proposals[i]]\n",
        "        proposals[i] += 1\n",
        "\n",
        "        if len(task_assignments[j]) < capacity:\n",
        "            worker_assignments[i].append(j)\n",
        "            task_assignments[j].append(i)\n",
        "        else:\n",
        "            # Find worst assigned worker\n",
        "            current_utils = [t_utils[j, k] for k in task_assignments[j]]\n",
        "            min_util = min(current_utils)\n",
        "            worst_idx = task_assignments[j][np.argmin(current_utils)]\n",
        "\n",
        "            if t_utils[j, i] > min_util:\n",
        "                # Replace worker\n",
        "                worker_assignments[worst_idx].remove(j)\n",
        "                task_assignments[j].remove(worst_idx)\n",
        "                unmatched.append(worst_idx)\n",
        "\n",
        "                worker_assignments[i].append(j)\n",
        "                task_assignments[j].append(i)\n",
        "            else:\n",
        "                unmatched.append(i)\n",
        "\n",
        "    return worker_assignments, task_assignments\n",
        "\n",
        "def random_matching(workers, tasks, capacity=CAPACITY):\n",
        "    worker_assignments = [[] for _ in range(len(workers))]\n",
        "    task_assignments = [[] for _ in range(len(tasks))]\n",
        "    all_pairs = [(i, j) for i in range(len(workers)) for j in range(len(tasks))]\n",
        "    random.shuffle(all_pairs)\n",
        "\n",
        "    for i, j in all_pairs:\n",
        "        if (len(worker_assignments[i]) < capacity and\n",
        "            len(task_assignments[j]) < capacity and\n",
        "            np.all(workers[i].skill >= tasks[j].required_skill)):\n",
        "            worker_assignments[i].append(j)\n",
        "            task_assignments[j].append(i)\n",
        "\n",
        "    return worker_assignments, task_assignments\n",
        "\n",
        "def fml_matching(workers, tasks, capacity=CAPACITY):\n",
        "    avg_weights = np.mean([w.weights for w in workers], axis=0)\n",
        "    for worker in workers:\n",
        "        worker.weights = avg_weights.copy()\n",
        "    return classic_gs(workers, tasks, capacity)\n",
        "\n",
        "############################\n",
        "### 6. Evaluation Metrics ##\n",
        "############################\n",
        "def calculate_satisfaction(workers, tasks, assignments):\n",
        "    total_util = 0\n",
        "    count = 0\n",
        "    for i, worker in enumerate(workers):\n",
        "        if assignments[i]:\n",
        "            utils = [worker_utility(worker, tasks[j]) for j in assignments[i]]\n",
        "            total_util += np.mean(utils)\n",
        "            count += 1\n",
        "    return total_util / count if count else 0\n",
        "\n",
        "def worker_utility(worker, task):\n",
        "    tau = worker.travel_time(task.location)\n",
        "    skill_gap = np.mean(worker.skill - task.required_skill)\n",
        "    return (worker.weights[0] * (1/tau) +\n",
        "            worker.weights[1] * task.reward +\n",
        "            worker.weights[2] * skill_gap +\n",
        "            0.3 * worker.reputation)\n",
        "\n",
        "def calculate_task_coverage(task_assignments):\n",
        "    return sum(len(a) > 0 for a in task_assignments) / len(task_assignments)\n",
        "\n",
        "def calculate_quality(workers, tasks, assignments):\n",
        "    qualities = []\n",
        "    for i, worker in enumerate(workers):\n",
        "        for task_id in assignments[i]:\n",
        "            task = tasks[task_id]\n",
        "            skill_diff = worker.skill - task.required_skill\n",
        "            # Only consider positive skill differences\n",
        "            base_quality = np.mean(np.maximum(skill_diff, 0)) / 5\n",
        "            noise = np.random.normal(0, 0.2 * (1 - worker.reputation))\n",
        "            qualities.append(max(0, min(1, base_quality + noise)))\n",
        "    return np.mean(qualities) if qualities else 0\n",
        "\n",
        "def check_stability(workers, tasks, worker_assignments, task_assignments):\n",
        "    blocking_pairs = 0\n",
        "    sample_size = min(500, len(workers))\n",
        "\n",
        "    for i in np.random.choice(len(workers), sample_size, replace=False):\n",
        "        worker = workers[i]\n",
        "        current_utils = [worker_utility(worker, tasks[j]) for j in worker_assignments[i]]\n",
        "        current_min = min(current_utils) if current_utils else -np.inf\n",
        "\n",
        "        for j in np.random.choice(len(tasks), min(50, len(tasks)), replace=False):\n",
        "            if j in worker_assignments[i]:\n",
        "                continue\n",
        "            w_util = worker_utility(worker, tasks[j])\n",
        "            if w_util <= current_min:\n",
        "                continue\n",
        "            t_util = task_utility(tasks[j], worker)\n",
        "            current_workers = task_assignments[j]\n",
        "            if not current_workers:\n",
        "                blocking_pairs += 1\n",
        "                continue\n",
        "            min_current_util = min(task_utility(tasks[j], workers[k]) for k in current_workers)\n",
        "            if t_util > min_current_util:\n",
        "                blocking_pairs += 1\n",
        "\n",
        "    return 1 - (blocking_pairs / (sample_size * 50))\n",
        "\n",
        "def task_utility(task, worker):\n",
        "    if np.any(worker.skill < task.required_skill):\n",
        "        return 0\n",
        "    tau = worker.travel_time(task.location)\n",
        "    return 0.7 * (1/tau) + 0.3 * worker.reputation\n",
        "\n",
        "########################\n",
        "### 7. Main Simulation #\n",
        "########################\n",
        "def run_full_simulation(real_world=False):\n",
        "    print(f\"🚀 Starting {'real-world' if real_world else 'synthetic'} simulation\")\n",
        "    results = []\n",
        "    methods = {\n",
        "        \"POTA (Proposed)\": rags_matching,\n",
        "        \"Classic GS\": classic_gs,\n",
        "        \"Random Allocation\": random_matching,\n",
        "        \"FML\": fml_matching\n",
        "    }\n",
        "\n",
        "    # Generate or load data\n",
        "    if not real_world and SAVE_DATA:\n",
        "        data = load_synthetic_data()\n",
        "        if data:\n",
        "            workers, tasks = data\n",
        "            print(\"✅ Loaded saved synthetic data\")\n",
        "        else:\n",
        "            workers = [Worker(i, real_world) for i in tqdm(range(N_WORKERS), desc=\"Workers\")]\n",
        "            tasks = [Task(j, real_world) for j in tqdm(range(N_TASKS), desc=\"Tasks\")]\n",
        "            save_synthetic_data(workers, tasks)\n",
        "    else:\n",
        "        workers = [Worker(i, real_world) for i in tqdm(range(N_WORKERS), desc=\"Workers\")]\n",
        "        tasks = [Task(j, real_world) for j in tqdm(range(N_TASKS), desc=\"Tasks\")]\n",
        "\n",
        "    # Train DQRS model\n",
        "    print(\"Training DQRS VAE...\")\n",
        "    vae_model = QualityVAE().to(device)\n",
        "    optimizer = optim.Adam(vae_model.parameters(), lr=0.001)\n",
        "    n_samples = 10000 if DEMO_MODE else 100000\n",
        "    features = torch.randn(n_samples, 3, device=device)\n",
        "\n",
        "    for _ in tqdm(range(VAE_EPOCHS), desc=\"VAE Training\"):\n",
        "        recon, mu, logvar = vae_model(features)\n",
        "        loss = vae_loss(recon, features, mu, logvar)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Federated RL Training\n",
        "    print(\"Training federated RL agents...\")\n",
        "    worker_models = []\n",
        "    batch_size = min(100, len(workers))\n",
        "\n",
        "    # FIXED: Removed extra parenthesis in tqdm call\n",
        "    for i in tqdm(range(0, len(workers), batch_size), desc=\"Federated RL\"):\n",
        "        batch = workers[i:i+batch_size]\n",
        "        batch_models = [train_worker_rl(w, tasks) for w in batch]\n",
        "        worker_models.extend(batch_models)\n",
        "        if GPU_ACCELERATED:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Apply federated averaging\n",
        "    print(\"Applying federated averaging...\")\n",
        "    global_weights = federated_average(worker_models)\n",
        "    for i, model in enumerate(worker_models):\n",
        "        model.load_state_dict(global_weights)\n",
        "        with torch.no_grad():\n",
        "            input_tensor = torch.randn(1, 1, 5)\n",
        "            if GPU_ACCELERATED:\n",
        "                input_tensor = input_tensor.to(device)\n",
        "            weights = model(input_tensor).cpu().numpy()[0]\n",
        "            workers[i].weights = weights\n",
        "\n",
        "    # Run all matching algorithms\n",
        "    for method_name, matching_fn in methods.items():\n",
        "        print(f\"\\n🔍 Running {method_name}...\")\n",
        "        start_time = time.time()\n",
        "        for w in workers:\n",
        "            w.assigned_tasks = []\n",
        "        worker_assignments, task_assignments = matching_fn(workers, tasks)\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        # Calculate metrics\n",
        "        satisfaction = calculate_satisfaction(workers, tasks, worker_assignments)\n",
        "        coverage = calculate_task_coverage(task_assignments)\n",
        "        quality = calculate_quality(workers, tasks, worker_assignments)\n",
        "        stability = check_stability(workers, tasks, worker_assignments, task_assignments)\n",
        "\n",
        "        results.append({\n",
        "            \"Method\": method_name,\n",
        "            \"Satisfaction\": satisfaction,\n",
        "            \"Task Coverage\": coverage,\n",
        "            \"Data Quality\": quality,\n",
        "            \"Stability\": stability,\n",
        "            \"Time (s)\": elapsed,\n",
        "            \"Scenario\": \"Real-world\" if real_world else \"Synthetic\"\n",
        "        })\n",
        "\n",
        "        print(f\"  Satisfaction: {satisfaction:.3f} | Coverage: {coverage:.1%} | \"\n",
        "              f\"Quality: {quality:.3f} | Stability: {stability:.1%} | Time: {elapsed:.1f}s\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_parameter_sensitivity():\n",
        "    print(\"🔬 Running parameter sensitivity analysis...\")\n",
        "    results = []\n",
        "    workers = [Worker(i) for i in range(1000)]\n",
        "    tasks = [Task(j) for j in range(2000)]\n",
        "\n",
        "    # Test beta (reputation weight)\n",
        "    for beta in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
        "        start_time = time.time()\n",
        "        worker_assignments, _ = rags_matching(workers, tasks, beta=beta)\n",
        "        elapsed = time.time() - start_time\n",
        "        satisfaction = calculate_satisfaction(workers, tasks, worker_assignments)\n",
        "        quality = calculate_quality(workers, tasks, worker_assignments)\n",
        "        results.append({\"Parameter\": \"β\", \"Value\": beta, \"Satisfaction\": satisfaction, \"Quality\": quality, \"Time (s)\": elapsed})\n",
        "\n",
        "    # Test learning rate (eta)\n",
        "    for lr in [0.001, 0.005, 0.01, 0.05, 0.1]:\n",
        "        worker_models = [train_worker_rl(w, tasks, lr=lr) for w in workers]\n",
        "        global_weights = federated_average(worker_models)\n",
        "        for i, model in enumerate(worker_models):\n",
        "            model.load_state_dict(global_weights)\n",
        "            with torch.no_grad():\n",
        "                input_tensor = torch.randn(1, 1, 5)\n",
        "                if GPU_ACCELERATED:\n",
        "                    input_tensor = input_tensor.to(device)\n",
        "                weights = model(input_tensor).cpu().numpy()[0]\n",
        "                workers[i].weights = weights\n",
        "        start_time = time.time()\n",
        "        worker_assignments, _ = rags_matching(workers, tasks)\n",
        "        elapsed = time.time() - start_time\n",
        "        satisfaction = calculate_satisfaction(workers, tasks, worker_assignments)\n",
        "        quality = calculate_quality(workers, tasks, worker_assignments)\n",
        "        results.append({\"Parameter\": \"η\", \"Value\": lr, \"Satisfaction\": satisfaction, \"Quality\": quality, \"Time (s)\": elapsed})\n",
        "\n",
        "    # Test alpha (reputation decay)\n",
        "    original_reps = [w.reputation for w in workers]\n",
        "    for alpha in [0.6, 0.7, 0.8, 0.9, 0.95]:\n",
        "        for w in workers:\n",
        "            w.reputation *= alpha\n",
        "        start_time = time.time()\n",
        "        worker_assignments, _ = rags_matching(workers, tasks)\n",
        "        elapsed = time.time() - start_time\n",
        "        satisfaction = calculate_satisfaction(workers, tasks, worker_assignments)\n",
        "        quality = calculate_quality(workers, tasks, worker_assignments)\n",
        "        results.append({\"Parameter\": \"α\", \"Value\": alpha, \"Satisfaction\": satisfaction, \"Quality\": quality, \"Time (s)\": elapsed})\n",
        "        for w, rep in zip(workers, original_reps):\n",
        "            w.reputation = rep\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def plot_results(results_df, sensitivity_df, scenario):\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    plt.subplot(2, 2, 1)\n",
        "    sns.barplot(x=\"Method\", y=\"Satisfaction\", data=results_df)\n",
        "    plt.title(f\"Worker Satisfaction ({scenario})\")\n",
        "    plt.ylim(0, 10)\n",
        "    plt.xticks(rotation=15)\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    sns.barplot(x=\"Method\", y=\"Data Quality\", data=results_df)\n",
        "    plt.title(f\"Data Quality ({scenario})\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xticks(rotation=15)\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    sns.barplot(x=\"Method\", y=\"Task Coverage\", data=results_df)\n",
        "    plt.title(f\"Task Coverage ({scenario})\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xticks(rotation=15)\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    sns.barplot(x=\"Method\", y=\"Stability\", data=results_df)\n",
        "    plt.title(f\"Matching Stability ({scenario})\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xticks(rotation=15)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"comparison_{scenario.lower().replace(' ', '_')}.png\")\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    for i, param in enumerate(['β', 'η', 'α']):\n",
        "        plt.subplot(2, 2, i+1)\n",
        "        param_df = sensitivity_df[sensitivity_df['Parameter'] == param]\n",
        "        plt.plot(param_df['Value'], param_df['Satisfaction'], 'o-', label='Satisfaction')\n",
        "        plt.plot(param_df['Value'], param_df['Quality'], 's-', label='Quality')\n",
        "        plt.title(f\"Sensitivity to {param}\")\n",
        "        plt.xlabel(\"Parameter Value\")\n",
        "        plt.ylabel(\"Metric Value\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"parameter_sensitivity.png\")\n",
        "    print(\"\\n📊 Visualizations saved\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Run synthetic scenario\n",
        "    synthetic_results = run_full_simulation(real_world=False)\n",
        "    synthetic_df = pd.DataFrame(synthetic_results)\n",
        "\n",
        "    # Run real-world scenario\n",
        "    realworld_results = run_full_simulation(real_world=True)\n",
        "    realworld_df = pd.DataFrame(realworld_results)\n",
        "\n",
        "    # Run sensitivity analysis\n",
        "    sensitivity_df = run_parameter_sensitivity()\n",
        "\n",
        "    # Combine and save results\n",
        "    results_df = pd.concat([synthetic_df, realworld_df])\n",
        "    results_df.to_csv(\"pota_results.csv\", index=False)\n",
        "    sensitivity_df.to_csv(\"parameter_sensitivity.csv\", index=False)\n",
        "    print(\"\\n✅ Results saved to CSV files\")\n",
        "\n",
        "    # Visualize results\n",
        "    plot_results(synthetic_df, sensitivity_df, \"Synthetic\")\n",
        "    plot_results(realworld_df, sensitivity_df, \"Real-world\")\n",
        "\n",
        "    # Display final results\n",
        "    print(\"\\n⭐ Final Results ⭐\")\n",
        "    print(results_df.groupby(['Scenario', 'Method']).mean())\n",
        "    print(f\"\\n🕒 Total execution time: {time.time()-start_time:.1f} seconds\")\n"
      ]
    }
  ]
}